zookeeper 安装
1 配置 /etc/hosts ,所有集群主机可以相互 ping 通
2 安装 java-1.8.0-openjdk-devel
3 zookeeper 解压拷贝到 /usr/local/zookeeper
4 配置文件改名，并在最后添加配置
mv zoo_sample.cfg  zoo.cfg
zoo.cfg 最后添加配置
server.1=node1:2888:3888
server.2=node2:2888:3888
server.3=node3:2888:3888
server.4=nn01:2888:3888:observer
5 拷贝 /usr/local/zookeeper 到其他集群主机
6 创建 mkdir /tmp/zookeeper
ALL: 7 创建 myid 文件，id 必须与配置文件里主机名对应的 server.(id) 一致 echo 1 >
/tmp/zookeeper/myid
8 启动服务，单启动一台无法查看状态，需要启动全部集群以后才能查看状态
/usr/local/zookeeper/bin/zkServer.sh start
9 查看状态
/usr/local/zookeeper/bin/zkServer.sh status

利用 api 查看状态的脚本
#!/bin/bash
function getstatus(){
    exec 9<>/dev/tcp/$1/2181 2>/dev/null
    echo stat >&9
    MODE=$(cat <&9 |grep -Po "(?<=Mode:).*")
    exec 9<&-
    echo ${MODE:-NULL}
}
for i in node{1..3} nn01;do
    echo -ne "${i}\t"
    getstatus ${i}
done

kafka 搭建
1 下载解压 kafka 压缩包
2 把 kafka 拷贝到 /usr/local/kafka 下面
3 修改配置文件 /usr/local/kafka/config/server.properties
broker.id=11
zookeeper.connect=node1:2181,node2:2181,node3:2181
4 拷贝 kafka 到其他主机，并修改 broker.id ,不能重复
5 启动 kafka 集群
/usr/local/kafka/bin/kafka-server-start.sh -daemon
/usr/local/kafka/config/server.properties

验证集群
创建一个 topic 
./bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --zookeeper node3:2181 --topic nsd1803
生产者
./bin/kafka-console-producer.sh --broker-list node2:9092 --topic nsd1803
消费者
./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic nsd1803

hadoop 高可用
ALL: 配置 /etc/hosts
192.168.1.10	nn01
192.168.1.20	nn02
192.168.1.11	node1
192.168.1.12	node2
192.168.1.13	node3
ALL: 除了 zookeeper 其他 hadoop ，kafka 服务全部停止
ALL: 初始化 hdfs 集群，删除 /var/hadoop/*
NN2: 关闭 ssh key 验证，部署公钥私钥
StrictHostKeyChecking no
scp nn01:/root/.ssh/id_rsa /root/.ssh/
scp nn01:/root/.ssh/authorized_keys /root/.ssh/

NN1: 配置 core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://nsdcluster</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/hadoop</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.nsd1803.hosts</name>
        <value>*</value>
    </property>
</configuration>

配置 hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.nameservices</name>
        <value>nsdcluster</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.nsdcluster</name>
        <value>nn1,nn2</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.nsdcluster.nn1</name>
        <value>nn01:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.nsdcluster.nn2</name>
        <value>nn02:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.nsdcluster.nn1</name>
        <value>nn01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.nsdcluster.nn2</name>
        <value>nn02:50070</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://node1:8485;node2:8485;node3:8485/nsdcluster</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/var/hadoop/journal</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.nsdcluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
</configuration>

yarn-site.xml 配置文件
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property> 
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>node1:2181,node2:2181,node3:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarn-ha</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>nn01</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>nn02</value>
    </property>
</configuration>

#-----------------------------------------------------#
初始化启动集群
ALL: 所有机器
nodeX： node1    node2    node3
NN1: nn01
NN2: nn02
#-----------------------------------------------------#
ALL:  同步配置到所有集群机器

NN1: 初始化ZK集群  ./bin/hdfs zkfc -formatZK

nodeX:  启动 journalnode 服务 
        ./sbin/hadoop-daemon.sh start journalnode

NN1: 格式化  ./bin/hdfs  namenode  -format

NN2: 数据同步到本地 /var/hadoop/dfs

NN1: 初始化 JNS
        ./bin/hdfs namenode -initializeSharedEdits

nodeX: 停止 journalnode 服务
        ./sbin/hadoop-daemon.sh stop journalnode

#-----------------------------------------------------#
启动集群
NN1: ./sbin/start-all.sh
NN2: ./sbin/yarn-daemon.sh start resourcemanager

查看集群状态
./bin/hdfs haadmin -getServiceState nn1  
./bin/hdfs haadmin -getServiceState nn2
./bin/yarn rmadmin -getServiceState rm1
./bin/yarn rmadmin -getServiceState rm2

./bin/hdfs dfsadmin -report
./bin/yarn  node  -list

访问集群：
./bin/hadoop  fs -ls  /
./bin/hadoop  fs -mkdir hdfs://nsdcluster/input

验证高可用，关闭 active namenode
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager

恢复节点
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager

